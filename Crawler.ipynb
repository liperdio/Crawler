{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7478f4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: http://accent.gmu.edu/browse_language.php?function=detail&speakerid=3​\n",
      "Crawling: http://www.ethnologue.com/show_language.asp?code=any\n",
      "Crawling: https://cam.ethnologue.com/\n",
      "Crawling: https://www.facebook.com/theEthnologue/\n",
      "Crawling: https://twitter.com/theEthnologue\n",
      "Crawling: https://github.com/sillsdev\n",
      "Crawling: https://www.sil.org/language-services\n",
      "Crawling: https://www.derivation.co\n",
      "Crawling: https://www.sil.org/contact\n",
      "Crawling: https://www.facebook.com/SIL.International\n",
      "Crawling: https://twitter.com/SILintl\n",
      "Crawling: https://www.instagram.com/sil_international/\n",
      "Crawling: https://www.linkedin.com/company/sil-international\n",
      "Crawling: https://www.youtube.com/channel/UC6ojqR9jOPHuPN7XP92Zj9Q\n",
      "Crawling: https://github.com/silinternational\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def web_crawler(url, max_depth):\n",
    "    visited_urls = set()\n",
    "\n",
    "    def crawl(url, depth):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "\n",
    "        print(\"Crawling:\", url)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            # Handle any exceptions that occur during the request\n",
    "            return\n",
    "\n",
    "        visited_urls.add(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract all links on the page\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if href and href.startswith('http') and href not in visited_urls:\n",
    "                crawl(href, depth + 1)\n",
    "\n",
    "    crawl(url, 0)\n",
    "\n",
    "# Example usage\n",
    "web_crawler('http://accent.gmu.edu/browse_language.php?function=detail&speakerid=3​', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f87e0561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: http://accent.gmu.edu/browse_language.php?function=detail&speakerid=3​\n",
      "Crawling: http://www.ethnologue.com/show_language.asp?code=any\n",
      "Crawling: https://cam.ethnologue.com/\n",
      "Crawling: https://www.facebook.com/theEthnologue/\n",
      "Crawling: https://twitter.com/theEthnologue\n",
      "Crawling: https://github.com/sillsdev\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Corg-login%3E&source=header'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m     crawl(url, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[43mweb_crawler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp://accent.gmu.edu/browse_language.php?function=detail&speakerid=3​\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 48\u001b[0m, in \u001b[0;36mweb_crawler\u001b[1;34m(url, max_depth)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException:\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;66;03m# Handle any exceptions that occur during the file download\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     25\u001b[0m     href \u001b[38;5;241m=\u001b[39m link\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m href \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited_urls:\n\u001b[1;32m---> 27\u001b[0m         \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Download files\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_link \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[3], line 27\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     25\u001b[0m     href \u001b[38;5;241m=\u001b[39m link\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m href \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited_urls:\n\u001b[1;32m---> 27\u001b[0m         \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Download files\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_link \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[3], line 34\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     33\u001b[0m     file_url \u001b[38;5;241m=\u001b[39m urljoin(url, href)\n\u001b[1;32m---> 34\u001b[0m     \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.download_file\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     38\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     39\u001b[0m filename \u001b[38;5;241m=\u001b[39m url\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Corg-login%3E&source=header'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "\n",
    "def web_crawler(url, max_depth):\n",
    "    visited_urls = set()\n",
    "\n",
    "    def crawl(url, depth):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "\n",
    "        print(\"Crawling:\", url)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            # Handle any exceptions that occur during the request\n",
    "            return\n",
    "\n",
    "        visited_urls.add(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract all links on the page\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if href and href.startswith('http') and href not in visited_urls:\n",
    "                crawl(href, depth + 1)\n",
    "\n",
    "        # Download files\n",
    "        for file_link in soup.find_all('a'):\n",
    "            href = file_link.get('href')\n",
    "            if href and not href.startswith('http'):\n",
    "                file_url = urljoin(url, href)\n",
    "                download_file(file_url)\n",
    "\n",
    "    def download_file(url):\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            filename = url.split('/')[-1]\n",
    "            with open(filename, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        file.write(chunk)\n",
    "        except requests.exceptions.RequestException:\n",
    "            # Handle any exceptions that occur during the file download\n",
    "            return\n",
    "\n",
    "    crawl(url, 0)\n",
    "\n",
    "# Example usage\n",
    "web_crawler('http://accent.gmu.edu/browse_language.php?function=detail&speakerid=3​', 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0843a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: http://accent.gmu.edu/browse_language.php?function=detail&speakerid=3​\n",
      "Crawling: http://www.ethnologue.com/show_language.asp?code=any\n",
      "Crawling: https://cam.ethnologue.com/\n",
      "Crawling: https://www.facebook.com/theEthnologue/\n",
      "Crawling: https://twitter.com/theEthnologue\n",
      "Crawling: https://github.com/sillsdev\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m     crawl(url, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[43mweb_crawler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp://accent.gmu.edu/browse_language.php?function=detail&speakerid=3​\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 49\u001b[0m, in \u001b[0;36mweb_crawler\u001b[1;34m(url, max_depth)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;66;03m# Handle any exceptions that occur during the file download\u001b[39;00m\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     26\u001b[0m     href \u001b[38;5;241m=\u001b[39m link\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m href \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited_urls:\n\u001b[1;32m---> 28\u001b[0m         \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Download files\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_link \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     26\u001b[0m     href \u001b[38;5;241m=\u001b[39m link\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m href \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited_urls:\n\u001b[1;32m---> 28\u001b[0m         \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Download files\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_link \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     34\u001b[0m     file_url \u001b[38;5;241m=\u001b[39m urljoin(url, href)\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 41\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.download_file\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     39\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     40\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(urlparse(url)\u001b[38;5;241m.\u001b[39mpath)\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4175087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: http://accent.gmu.edu/browse_language.php?function=detail&speakerid=3​\n",
      "Crawling: http://www.ethnologue.com/show_language.asp?code=any\n",
      "Crawling: https://cam.ethnologue.com/\n",
      "Crawling: https://www.facebook.com/theEthnologue/\n",
      "Crawling: https://twitter.com/theEthnologue\n",
      "Crawling: https://github.com/sillsdev\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\LIPerdio\\\\Downloads\\\\'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m     crawl(url, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[43mweb_crawler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp://accent.gmu.edu/browse_language.php?function=detail&speakerid=3​\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 52\u001b[0m, in \u001b[0;36mweb_crawler\u001b[1;34m(url, max_depth)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException:\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;66;03m# Handle any exceptions that occur during the file download\u001b[39;00m\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 28\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     26\u001b[0m     href \u001b[38;5;241m=\u001b[39m link\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m href \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited_urls:\n\u001b[1;32m---> 28\u001b[0m         \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Download files\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_link \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[8], line 28\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     26\u001b[0m     href \u001b[38;5;241m=\u001b[39m link\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m href \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited_urls:\n\u001b[1;32m---> 28\u001b[0m         \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Download files\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_link \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[8], line 35\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     34\u001b[0m     file_url \u001b[38;5;241m=\u001b[39m urljoin(url, href)\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 44\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.download_file\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     42\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(directory, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Create the directory if it doesn't exist\u001b[39;00m\n\u001b[0;32m     43\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, filename)\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\LIPerdio\\\\Downloads\\\\'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import os\n",
    "\n",
    "def web_crawler(url, max_depth):\n",
    "    visited_urls = set()\n",
    "\n",
    "    def crawl(url, depth):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "\n",
    "        print(\"Crawling:\", url)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            # Handle any exceptions that occur during the request\n",
    "            return\n",
    "\n",
    "        visited_urls.add(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract all links on the page\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if href and href.startswith('http') and href not in visited_urls:\n",
    "                crawl(href, depth + 1)\n",
    "\n",
    "        # Download files\n",
    "        for file_link in soup.find_all('a'):\n",
    "            href = file_link.get('href')\n",
    "            if href and not href.startswith('http'):\n",
    "                file_url = urljoin(url, href)\n",
    "                download_file(file_url)\n",
    "\n",
    "    def download_file(url):\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            filename = os.path.basename(urlparse(url).path)\n",
    "            directory = r\"C:\\Users\\LIPerdio\\Downloads\"  # Specify the destination directory here\n",
    "            os.makedirs(directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        file.write(chunk)\n",
    "        except requests.exceptions.RequestException:\n",
    "            # Handle any exceptions that occur during the file download\n",
    "            return\n",
    "\n",
    "    crawl(url, 0)\n",
    "\n",
    "# Example usage\n",
    "web_crawler('http://accent.gmu.edu/browse_language.php?function=detail&speakerid=3​', 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f50280c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: http://accent.gmu.edu/index.php\n",
      "Crawling: http://www.facebook.com/speech.accent.archive\n",
      "Crawling: https://twitter.com/share\n",
      "Crawling: http://www.gmu.edu\n",
      "Crawling: https://www.sucuri.net/?utm_source=firewall_block\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\LIPerdio\\\\Downloads\\\\'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m     crawl(url, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[43mweb_crawler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp://accent.gmu.edu/index.php\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 52\u001b[0m, in \u001b[0;36mweb_crawler\u001b[1;34m(url, max_depth)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException:\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;66;03m# Handle any exceptions that occur during the file download\u001b[39;00m\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 28\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     26\u001b[0m     href \u001b[38;5;241m=\u001b[39m link\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m href \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited_urls:\n\u001b[1;32m---> 28\u001b[0m         \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Download files\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_link \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[10], line 28\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     26\u001b[0m     href \u001b[38;5;241m=\u001b[39m link\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m href \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited_urls:\n\u001b[1;32m---> 28\u001b[0m         \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Download files\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_link \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[10], line 35\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     34\u001b[0m     file_url \u001b[38;5;241m=\u001b[39m urljoin(url, href)\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 44\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.download_file\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     42\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(directory, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Create the directory if it doesn't exist\u001b[39;00m\n\u001b[0;32m     43\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, filename)\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\LIPerdio\\\\Downloads\\\\'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import os\n",
    "\n",
    "def web_crawler(url, max_depth):\n",
    "    visited_urls = set()\n",
    "\n",
    "    def crawl(url, depth):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "\n",
    "        print(\"Crawling:\", url)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            # Handle any exceptions that occur during the request\n",
    "            return\n",
    "\n",
    "        visited_urls.add(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract all links on the page\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if href and href.startswith('http') and href not in visited_urls:\n",
    "                crawl(href, depth + 1)\n",
    "\n",
    "        # Download files\n",
    "        for file_link in soup.find_all('a'):\n",
    "            href = file_link.get('href')\n",
    "            if href and not href.startswith('http'):\n",
    "                file_url = urljoin(url, href)\n",
    "                download_file(file_url)\n",
    "\n",
    "    def download_file(url):\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            filename = os.path.basename(urlparse(url).path)\n",
    "            directory = r\"C:\\Users\\LIPerdio\\Downloads\"  # Specify the destination directory here\n",
    "            os.makedirs(directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        file.write(chunk)\n",
    "        except requests.exceptions.RequestException:\n",
    "            # Handle any exceptions that occur during the file download\n",
    "            return\n",
    "\n",
    "    crawl(url, 0)\n",
    "\n",
    "# Example usage\n",
    "web_crawler('http://accent.gmu.edu/index.php', 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb1d021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: http://accent.gmu.edu/index.php\n",
      "Crawling: http://www.facebook.com/speech.accent.archive\n",
      "Crawling: https://twitter.com/share\n",
      "Crawling: http://www.gmu.edu\n",
      "Crawling: https://www.sucuri.net/?utm_source=firewall_block\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'downloads\\\\'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m     crawl(url, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[43mweb_crawler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp://accent.gmu.edu/index.php\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 52\u001b[0m, in \u001b[0;36mweb_crawler\u001b[1;34m(url, max_depth)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException:\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;66;03m# Handle any exceptions that occur during the file download\u001b[39;00m\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 28\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     26\u001b[0m     href \u001b[38;5;241m=\u001b[39m link\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m href \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited_urls:\n\u001b[1;32m---> 28\u001b[0m         \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Download files\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_link \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[11], line 28\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     26\u001b[0m     href \u001b[38;5;241m=\u001b[39m link\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m href \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited_urls:\n\u001b[1;32m---> 28\u001b[0m         \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Download files\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_link \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[11], line 35\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     34\u001b[0m     file_url \u001b[38;5;241m=\u001b[39m urljoin(url, href)\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 44\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.download_file\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     42\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(directory, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Create the directory if it doesn't exist\u001b[39;00m\n\u001b[0;32m     43\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, filename)\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'downloads\\\\'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import os\n",
    "\n",
    "def web_crawler(url, max_depth):\n",
    "    visited_urls = set()\n",
    "\n",
    "    def crawl(url, depth):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "\n",
    "        print(\"Crawling:\", url)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            # Handle any exceptions that occur during the request\n",
    "            return\n",
    "\n",
    "        visited_urls.add(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract all links on the page\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if href and href.startswith('http') and href not in visited_urls:\n",
    "                crawl(href, depth + 1)\n",
    "\n",
    "        # Download files\n",
    "        for file_link in soup.find_all('a'):\n",
    "            href = file_link.get('href')\n",
    "            if href and not href.startswith('http'):\n",
    "                file_url = urljoin(url, href)\n",
    "                download_file(file_url)\n",
    "\n",
    "    def download_file(url):\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            filename = os.path.basename(urlparse(url).path)\n",
    "            directory = \"downloads\"  # Save files in the 'downloads' directory within the current working directory\n",
    "            os.makedirs(directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        file.write(chunk)\n",
    "        except requests.exceptions.RequestException:\n",
    "            # Handle any exceptions that occur during the file download\n",
    "            return\n",
    "\n",
    "    crawl(url, 0)\n",
    "\n",
    "# Example usage\n",
    "web_crawler('http://accent.gmu.edu/index.php', 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41fe3894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: http://accent.gmu.edu/index.php\n",
      "Crawling: http://www.facebook.com/speech.accent.archive\n",
      "Crawling: https://twitter.com/share\n",
      "Crawling: http://www.gmu.edu\n",
      "Crawling: https://www.sucuri.net/?utm_source=firewall_block\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'downloads\\\\'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m     crawl(url, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[43mweb_crawler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp://accent.gmu.edu/index.php\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 52\u001b[0m, in \u001b[0;36mweb_crawler\u001b[1;34m(url, max_depth)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException:\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;66;03m# Handle any exceptions that occur during the file download\u001b[39;00m\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 28\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     26\u001b[0m     href \u001b[38;5;241m=\u001b[39m link\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m href \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited_urls:\n\u001b[1;32m---> 28\u001b[0m         \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Download files\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_link \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[13], line 28\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     26\u001b[0m     href \u001b[38;5;241m=\u001b[39m link\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m href \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m visited_urls:\n\u001b[1;32m---> 28\u001b[0m         \u001b[43mcrawl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Download files\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_link \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "Cell \u001b[1;32mIn[13], line 35\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.crawl\u001b[1;34m(url, depth)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m href \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     34\u001b[0m     file_url \u001b[38;5;241m=\u001b[39m urljoin(url, href)\n\u001b[1;32m---> 35\u001b[0m     \u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 44\u001b[0m, in \u001b[0;36mweb_crawler.<locals>.download_file\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     42\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(directory, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Create the directory if it doesn't exist\u001b[39;00m\n\u001b[0;32m     43\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, filename)\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m):\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'downloads\\\\'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import os\n",
    "\n",
    "def web_crawler(url, max_depth):\n",
    "    visited_urls = set()\n",
    "\n",
    "    def crawl(url, depth):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "\n",
    "        print(\"Crawling:\", url)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            # Handle any exceptions that occur during the request\n",
    "            return\n",
    "\n",
    "        visited_urls.add(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract all links on the page\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if href and href.startswith('http') and href not in visited_urls:\n",
    "                crawl(href, depth + 1)\n",
    "\n",
    "        # Download files\n",
    "        for file_link in soup.find_all('a'):\n",
    "            href = file_link.get('href')\n",
    "            if href and not href.startswith('http'):\n",
    "                file_url = urljoin(url, href)\n",
    "                download_file(file_url)\n",
    "\n",
    "    def download_file(url):\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            filename = os.path.basename(urlparse(url).path)\n",
    "            directory = \"downloads\"\n",
    "            os.makedirs(directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        file.write(chunk)\n",
    "        except requests.exceptions.RequestException:\n",
    "            # Handle any exceptions that occur during the file download\n",
    "            return\n",
    "\n",
    "    crawl(url, 0)\n",
    "\n",
    "# Example usage\n",
    "web_crawler('http://accent.gmu.edu/index.php', 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f052e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\liperdio\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\liperdio\\anaconda3\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\liperdio\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\liperdio\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\liperdio\\anaconda3\\lib\\site-packages (from requests) (2023.5.7)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\liperdio\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\liperdio\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Crawling: http://accent.gmu.edu/index.php\n",
      "Crawling: http://www.facebook.com/speech.accent.archive\n",
      "Crawling: https://twitter.com/share\n",
      "Crawling: http://www.gmu.edu\n",
      "Crawling: https://www.sucuri.net/?utm_source=firewall_block\n",
      "Crawling: https://sucuri.net/?utm_source=firewall_block\n",
      "Crawling: https://support.sucuri.net/?utm_source=firewall_block\n",
      "Crawling: https://sucuri.net/privacy-policy?utm_source=firewall_block\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import os\n",
    "\n",
    "def web_crawler(url, max_depth):\n",
    "    visited_urls = set()\n",
    "\n",
    "    def crawl(url, depth):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "\n",
    "        print(\"Crawling:\", url)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            # Handle any exceptions that occur during the request\n",
    "            return\n",
    "\n",
    "        visited_urls.add(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Download MP3 files\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if href and href.endswith('.mp3'):\n",
    "                audio_url = urljoin(url, href)\n",
    "                download_file(audio_url)\n",
    "\n",
    "        # Extract all links on the page\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if href and href.startswith('http') and href not in visited_urls:\n",
    "                crawl(href, depth + 1)\n",
    "\n",
    "    def download_file(url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            filename = os.path.basename(urlparse(url).path)\n",
    "            directory = \"/content/mp3_files\"  # Specify the destination directory here\n",
    "            os.makedirs(directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "            file_path = os.path.join(directory, filename)\n",
    "\n",
    "            if os.path.isfile(file_path):\n",
    "                # If the file already exists, skip downloading\n",
    "                return\n",
    "\n",
    "            with open(file_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        except requests.exceptions.RequestException:\n",
    "            # Handle any exceptions that occur during the file download\n",
    "            return\n",
    "\n",
    "    crawl(url, 0)\n",
    "\n",
    "# Example usage\n",
    "web_crawler('http://accent.gmu.edu/index.php', 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bae3510a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: http://accent.gmu.edu/soundtracks/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import os\n",
    "\n",
    "def web_crawler(url, max_depth):\n",
    "    visited_urls = set()\n",
    "\n",
    "    def crawl(url, depth):\n",
    "        if depth > max_depth:\n",
    "            return\n",
    "\n",
    "        print(\"Crawling:\", url)\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "        except requests.exceptions.RequestException:\n",
    "            # Handle any exceptions that occur during the request\n",
    "            return\n",
    "\n",
    "        visited_urls.add(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract all links on the page\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if href and href.startswith('http') and href not in visited_urls:\n",
    "                crawl(href, depth + 1)\n",
    "\n",
    "        # Download MP3 files\n",
    "        for file_link in soup.find_all('a'):\n",
    "            href = file_link.get('href')\n",
    "            if href and href.endswith('.mp3'):\n",
    "                file_url = urljoin(url, href)\n",
    "                download_file(file_url)\n",
    "\n",
    "    def download_file(url):\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            filename = os.path.basename(urlparse(url).path)\n",
    "            file_path = os.path.join(\"mp3_files\", filename)\n",
    "\n",
    "            if os.path.isfile(file_path):\n",
    "                return\n",
    "\n",
    "            with open(file_path, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "        except requests.exceptions.RequestException:\n",
    "            # Handle any exceptions that occur during the file download\n",
    "            return\n",
    "\n",
    "    crawl(url, 0)\n",
    "\n",
    "# Specify the destination directory here\n",
    "os.makedirs(\"mp3_files\", exist_ok=True)\n",
    "\n",
    "# Example usage\n",
    "web_crawler('http://accent.gmu.edu/soundtracks/', 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32299857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
